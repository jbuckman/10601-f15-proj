\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_e,times}
\usepackage{hyperref}
\usepackage{url}
% \documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09


\title{10-601 Midway Report}


\author{
Jacob Buckman \\
School of Computer Science\\
Carnegie Mellon University\\
Pittsburgh, PA 15213 \\
\texttt{jacobbuckman@cmu.edu} \\
\And
John Corbett \\
School of Computer Science \\
Carnegie Mellon University \\
Pittsburgh, PA 15213 \\
\texttt{jtcorbett@cmu.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\section{Introduction}

The goal of this project is to devise a machine learning classifier that performs well on the CIFAR-100 dataset. This dataset contains $50,000$ labeled images divided into $100$ classes and $20$ superclasses. To classify these images we will try three different machine learning approaches: a Gaussian Na\"ive Bayes classifier (GNB), a Neural Network (NN), and a Random Forest (RF). We will then withhold a subset of the training data, train each of the classifiers on the remaining data, and see how it scores on the withheld data. This process is known as cross-validation. We aim to achieve better than baseline accuracy\footnote{We will take baseline accuracy to be 48\%.} with at least one of the aforementioned approaches.

\section{Methods}

Since the images that we are classifying are only $32 \times 32$ pixels, take each pixel to a feature for training the classifiers for a total of $3072$ features. Before running the classifiers on the image data, it may be useful to run feature extraction which will train the classifiers on the features of the images rather than the raw image data. We will have a preprocessing step where feature extraction occurs, then train the classifiers on the preprocessed data rather than the raw data. In addition to training the classifiers on the raw data (RGB pixel values of the image), we will also train it on the HSV pixel values of the image, the vectorized HOG descriptor of the image obtained using the \verb|VLFeat| library, the regional average pixel color and number of edges and corners detected.

Given that the CIFAR-100 dataset contains a large about of images and these classifiers take a long time to train and to run, when determining the efficiency of the classifiers, we will operate on the CIFAR-10 dataset under the assumption that success in classifying this dataset will directly translate to success in classifying the CIFAR-100 dataset.

\subsection*{Gaussian Na\"ive Bayes}

We choose to use a GNB classifier over a regular Na\"ive Bayes classifier because even though the data points are discrete, they can take on a wide range of values in a continuous fashion which lends itself more to the Gaussian version of the classifier. The GNB classifier was trained with and without feature detection and tested with cross-validation withholding 5\% and 10\% of the dataset. We use a maximum likelihood estimation when training GNB, so there is no fabricated data and the resulting model will maximize the likelihood of the training data occurring.

GNB relies on the assumption that each of the features are independent of each other. Given that this is clearly a faulty assumption, we do not expect that this classifier will perform as well as the others, but it given that it runs very quickly, we can use this classifier to quickly test the efficiency of other parts of our model such as feature extraction. Our implementation of GNB does \textit{not} assume that the variance is independent of the training samples or labels.

\subsection*{Neural Network}

\textbf{TODO}

\section{Results}

As expected, the Neural Network greatly outperformed the GNB in terms of accuracy, but the GNB performed much faster in both the training and the classifying steps. Interestingly, all classifiers performed better without feature detection.

\subsection*{Gaussian Na\"ive Bayes}

Without feature extraction, GNB was able to correctly classify just over 20\% of samples. This is about 10\% better than random guessing, but is still significantly below baseline accuracy. With feature detection (HSV and vhog), this accuracy drops to about 14\%, only slightly better than random guessing.

\subsection*{Neural Network}

\textbf{TODO}

\section{Conclusion}

\textbf{TODO: Possibly remove this section entirely for the Midway report}


\section{Contributions}

Jacob implemented the Neural Network and designed a testing script that allowed for easy comparison between the different algorithms. John implemented the Gaussian Na\"ive Bayes Classifier as well as the Random Forest and the HSV feature extractor. Both team members worked equally on this writeup.

\end{document}
